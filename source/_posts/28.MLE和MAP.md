---
title: MLE和MAP
date: 2019-02-03 7:09:17
categories: 
    - 数学
tag: 
    - 统计学
mathjax: true
---
频率学派 - Frequentist - Maximum Likelihood Estimation (MLE，最大似然估计)
贝叶斯学派 - Bayesian - Maximum A Posteriori (MAP，最大后验估计)

频率学派和贝叶斯学派对世界的认知有本质不同：
频率学派认为世界是确定的，有一个本体，这个本体的真值是不变的，我们的目标就是要找到这个真值或真值所在的范围；
而贝叶斯学派认为世界是不确定的，人们对世界先有一个预判，而后通过观测数据对这个预判做调整，我们的目标是要找到最优的描述这个世界的概率分布。

<!-- more -->

# 概率与统计的区别
概率（probabilty）和统计（statistics）研究的问题刚好相反。

概率：已知一个模型和参数，怎么去预测这个模型产生的结果的特性（例如均值，方差，协方差等等）。
统计：已知有一堆数据，要利用这堆数据去预测模型（什么分布）和参数（分布模型的参数）。

最大似然估计MLE，最大后验估计MAP，贝叶斯估计来说Bayes，都属于统计的范畴。

# 参数估计的方法
点估计就是用样本统计量的某一具体数值直接推断未知的总体参数。总体参数进行点估计常用的方法有两种：矩估计与最大似然估计，其中最大似然估计就是我们实际中使用非常广泛的一种方法。 

按这两种方法对总体参数进行点估计，能够得到相对准确的结果。如用样本均值X估计总体均值，或者用样本标准差S估计总体标准差σ。 

但是，点估计有一个不足之处，即这种估计方法不能提供估计参数的估计误差大小。对于一个总体来说，它的总体参数是一个常数值，而它的样本统计量却是随机变量。当用随机变量去估计常数值时，误差是不可避免的，只用一个样本数值去估计总体参数是要冒很大风险的。因为这种误差风险的存在，并且风险的大小还未知，所以，点估计主要为许多定性研究提供一定的参考数据，或在对总体参数要求不精确时使用，而在需要用精确总体参数的数据进行决策时则很少使用。

区间估计就是在推断总体参数时，还要根据统计量的抽样分布特征，估计出总体参数的一个区间，而不是一个数值，并同时给出总体参数落在这一区间的可能性大小，概率的保证。

# 什么是似然
The likelihood of something happening is how likely it is to happen.

$$P(x|\theta)$$
输入有两个：$x$表示某一个具体的数据；$\theta$表示模型的参数。

- 如果$\theta$是已知确定的，$x$是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点$x$，其出现概率是多少。
- 如果$x$是已知确定的，$\theta$是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现$x$这个样本点的概率是多少。

# 最大似然估计（MLE）
设有一个造币厂生产某种硬币，现在我们拿到了一枚这种硬币，想试试这硬币是不是均匀的。
于是我们拿这枚硬币抛了10次，得到的数据是：反正正正正反正正正反。我们想求的正面概率θθ是模型参数，而抛硬币模型我们可以假设是二项分布。
这是个只关于$θ$的函数。而最大似然估计，顾名思义，就是要最大化这个函数。
$$\displaystyle {\hat {\theta }}_{\mathrm {ML} }(x)=\arg \max _{\theta }f(x|\theta )\!$$

# 最大后验概率估计（MAP） 
最大似然估计是求参数$θ$, 使似然函数$P(x_0|θ)$最大。最大后验概率估计则是想求$θ$使$P(x_0|θ)P(θ)$最大。求得的$θ$不单单让似然函数大，$θ$自己出现的先验概率也得大。参数$θ$有一个先验概率。

这个时候就用到了我们的最大后验概率MAP，基础是贝叶斯公式： 
$$P(\theta|x_0) = \frac{P(x_0|\theta)P(\theta)}{P(x_0)}$$
其中，$p(x|θ)$就是之前讲的似然函数，$p(θ)$是先验概率，是指在没有任何实验数据的时候对参数$θ$的经验判断，对于一个硬币，大概率认为他是正常的，正面的概率为0.5的可能性最大。

MAP优化的就是一个后验概率，即给定了观测值以后使后验概率最大： 
$$
\begin{align*}
{\hat {\theta }}_{\mathrm {MAP} }  & = \arg \max _{\theta } p(\theta|x) \\
 & = \arg \max _{\theta } \frac{p(x|\theta)\times p(\theta)}{P(x)} \\
 & =  \arg \max _{\theta } p(x|\theta)\times p(\theta)
\end{align*}
$$
从上面公式可以看出，$p(x|θ)$是似然函数，而$p(θ)$就是先验概率。对其取对数： 
$$
\begin{align*}
\arg \max _{\theta } p(x|\theta) \cdot p(\theta) &= \arg \max _{\theta } log \prod_{i=0}^n p(x_i | \theta) p(\theta) \\
& = \arg \max _{\theta } \sum_i log (p(x_i | \theta) p(\theta)) \\ 
& = \arg \max _{\theta } \sum_ilog (p(x_i | \theta) + log(p(\theta))
\end{align*}
$$

# 贝叶斯估计
贝叶斯估计是在MAP上做进一步拓展，此时不直接估计参数的值，而是允许参数服从一定概率分布。回忆下贝叶斯公式： 
$$p(\theta|x) = \frac{p(x|\theta)\times p(\theta)}{P(x)}$$
现在我们不要求后验概率最大，这个时候就需要求$p(X)$，即观察到的$X$的概率。一般来说，用全概率公式可以求$p(X)$
$$p(X) = \int p(X | \theta)p(\theta)d\theta$$
那么如何用贝叶斯估计来预测呢？如果我们想求一个值 $x’$ 的概率，可以用下面的方法 
$$
p(\hat{x}|X)=\int_{\theta\in\Theta}p(\hat{x}|\theta)p(\theta|X)d\theta=\int_{\theta\in\Theta}\frac{p(\hat{x}|\theta)p(\theta)}{p(X)}d\theta
$$

# MLE和MAP的区别
当先验分布均匀之时，MAP与MLE相等。直观讲，它表征了最有可能值的任何先验知识的匮乏。在这一情况中，所有权重分配到似然函数，因此当我们把先验与似然相乘，由此得到的后验极其类似于似然。因此，最大似然方法可被看作一种特殊的MAP。

如果先验认为这个硬币是概率是均匀分布的，被称为无信息先验( non-informative prior )，通俗的说就是“让数据自己说话”，此时贝叶斯方法等同于频率方法。 
随着数据的增加，先验的作用越来越弱，数据的作用越来越强，参数的分布会向着最大似然估计靠拢。而且可以证明，最大后验估计的结果是先验和最大似然估计的凸组合。

重要结论：随着我们观测到越来越多的数据，我们从数据中获取的信息的置信度是越高的，MAP估计逐步逼近MLE，先验信息占的比重将会越来越少。

# 参考
### 这两篇博客把MLE和MAP的原理讲得很清楚：
[详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解](https://blog.csdn.net/u011508640/article/details/72815981)
[最大似然估计，最大后验估计，贝叶斯估计联系与区别](https://blog.csdn.net/bitcarmanlee/article/details/81417151)
### 这篇文章还讲了贝叶斯：
机器学习中的MLE、MAP、贝叶斯估计：https://zhuanlan.zhihu.com/p/37215276
### 这篇文章把频率学派和贝叶斯学派讲的很清楚：
聊一聊机器学习的MLE和MAP：最大似然估计和最大后验估计：https://zhuanlan.zhihu.com/p/32480810