---
title: Bayesian Theorem
date: 2018-10-02 16:42:38
categories: 
    - 数学
tag: 
    - 数学
mathjax: true
---
根据概率论中的贝叶斯公式，有：
$$
P(\omega_i|x)=\frac{p(x,\omega_i)}{p(x)}=\frac{p(x|\omega_i)p(\omega_i)}{p(x)},i=1,2,...
$$
- $p(\omega_i)$是先验概率
- $p(x,\omega_i)$是联合概率分布
- $p(x)$是总体密度
- $p(x|\omega_i)$是第$i$类x的概率密度，即类条件概率

这样，后验概率就转换成先验概率与类条件概率密度的乘积，再用总体密度进行归一化。这就是贝叶斯决策。

<!-- more -->

---
# 最小错误率准则
使错误率最小的分类决策，就是使后验概率最大。默认情况下，贝叶斯决策就是最小错误率决策。其中，后验概率用贝叶斯公式求得：
$$
P(\omega_i|x)=\frac{p(x|\omega_i)p(\omega_i)}{p(x)}=\frac{p(x|\omega_i)p(\omega_i)}{\sum_{j=1}^{2}{p(x|\omega_i)p(\omega_i)}},i=1,2
$$
最小错误率可以表示多种形式，比如：
$$
minP(e)=\int_{}^{}P(e)p(x)dx \\ 
P(\omega_i|x)=\max P(\omega_j|x),j=1,2
$$

如果$p(x|\omega_1)p(\omega_1)P(\omega_1)>p(x|\omega_2)p(\omega_2)P(\omega_2)$,则$x\in\omega_1$;反之，则$x\in\omega_2$
## 似然比
先验概率$p(\omega_i)$是事先确定的，与当前样本$x$无关，这样可以实现计算似然比$\lambda$，对每一个样本计算$l(x)$作比较
$$
l(x)=p(x|\omega_1)/p(x|\omega_2)，\lambda=P(\omega_2)/P(\omega_1) \\
l(x) > \lambda, x\in \omega_1 \\ 
l(x) < \lambda, x\in \omega_2
$$
为了计算，有时候使用对数似然比
$$
h(x)=-\ln [l(x)]=-\ln p(x|\omega_1)+\ln p(x|\omega_2)
$$
## 错误率
不同类的分界线称作决策边界，在多维情况下称为决策面，它把特征空间划分成属于各类的区域。
对二分类问题的错误率分析：
$$
\begin{equation} 
\begin{split} 
P(e)&= \int_{-\infty}^{t}P(\omega_2|x)p(x)dx+\int_{t}^{\infty}P(\omega_1|x)p(x)dx \\
&=\int_{-\infty}^{t}p(x|\omega_2)P(\omega_2)dx+\int_{t}^{\infty}p(x|\omega_1)P(\omega_1)dx \\
&=P(\omega_2)\int_{-\infty}^{t}p(x|\omega_2)dx+P(\omega_1)\int_{t}^{\infty}p(x|\omega_1)dx \\ 
&=P(\omega_2)P_2(e)+P(\omega_1)P_1(e)
\end{split} 
\end{equation}
$$
其中，$P_1(e)$是把第一类错判成第二类的错误率，$P_2(e)$是把第二类错判成第一类的错误类。
## 多决策
假如不是二分类，就要把特征空间分割成$\mathcal{R_1}，\mathcal{R_2}，...，\mathcal{R_n}$个区域，可能错分的情况就很多，平均错误率$P(e)$将有$c(c-1)$项，一共$c$行，每行$c-1$列。
$$
P(e)=\sum_{i=1}^{c}\sum_{j=1}^{c}{[P(x\in \mathcal{R_j}|\omega_i)]P(\omega_i)}
$$
可以通过计算平均正确率$P(c)$来降低计算量：
$$
P(c)=\sum_{j=1}^{c}{P(x\in\mathcal{R}|\omega_i)P(\omega_j)}=\sum_{j=1}^{c}{\int_{\mathcal{R_j}}p(x|\omega_j)P(\omega_j)}dx \\
P(e)=1-P(c)
$$

---
# 最小风险准则
所谓最小风险准则，就是考虑各种错误造成损失不同时的一种最优决策。
- 样本$x$看作$d$维随机向量：$x=[x_1,x_2,...,x_d]^T$
- 状态空间$\Omega$由$c$个可能的状态（$c$类）组成：$\Omega=\{\omega_1,\omega_2,...,\omega_c\}$
- 对$x$可能采取的决策组成决策空间，由$k$个决策组成：$\mathcal{A}=\{\alpha_1,\alpha_2,...,\alpha_k\}$
- 对实际为$\omega_j$的x采取决策$\alpha_i$所带来的损失为$\lambda(\alpha_i,\omega_j),i=1,...,k， j=1,...,c$

这里没有假定$k=c$，是因为允许拒绝决策，比如不属于任何一类，这是更一般的情况。决策损失可以列出一个$k$行$c$列的决策表。

对于某一个样本属于各个状态的后验概率是$P(\omega_j|x),j=1,...,c$，对它采取决策$\alpha_i,i=1,...,k$ 的期望损失是：
$$
R(\alpha_i|x)=E(\lambda(\alpha_i,\omega_j)|x)=\sum_{j=1}^{c}\lambda(\alpha_i,\omega_j)P(\omega_j|x)，i=1,...,k
$$
设某一个决策$\alpha(x)$，它对特征空间中所有可能的样本采用决策所造成的期望损失是$R(\alpha)$，称为平均/期望风险，令它最小就是最小风险准则。
$$
\min R(\alpha)=\int R(\alpha(x)|x)p(x)dx
$$
以二分类为例，
$$
\lambda_{11}P(\omega_1|x)+\lambda_{12}P(\omega_2|x) < \lambda_{21}P(\omega_1|x)+\lambda_{22}P(\omega_2|x)，x\in \omega_1 \\
\lambda_{11}P(\omega_1|x)+\lambda_{12}P(\omega_2|x) > \lambda_{21}P(\omega_1|x)+\lambda_{22}P(\omega_2|x)，x\in \omega_2
$$
关于决策表，可以写成矩阵吧，对角线一般都是0，最小错误率准则就是0-1决策表，而最小风险准则就是有了不同的参数.
$$
\left[\begin{matrix}\lambda_{11} & \lambda_{12} \\ \lambda_{21} & \lambda_{22}\end{matrix}\right]=\left[\begin{matrix}0 & 1 \\ 1 & 0\end{matrix}\right]
$$

---
# 两类错误率、Neyman-Pearson决策与ROC曲线

待续